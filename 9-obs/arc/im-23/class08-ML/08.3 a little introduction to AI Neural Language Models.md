
some assumptions
1. the bast majority of reasoning tasks can be efficiently expressed and evaluated in language.
2. the unsupervised learning is done by providing world's text and proccess making generative modeling.
3. The original WebText dataset was a web scrape of outbounds links from reddit through Deember 2017 (receiving at least 3 karma)
4. WebText 2 adde Jan-Oct 2018 links. (96 GB)
5. Later, a Books Corpus was added, Common Crawl, English Wikipedia and publicly-availabe Internet Books.
6. Since 2020 there is a new model every month:
	1. GPT-2
	2. BERT
	3. GPT-3(175B)
	4. LaMDA (137B)
	5. Jurassic-1(178B)
	6. Megatron-Turing NLG (530B)
	7. Gopher(280B)
7. En un nuevo artículo ("Training Compute-Optimal Large Language Models" de Hoffmann et al.), los investigadores de DeepMind revisaron las conclusiones de Kaplan y descubrieron que escalar el número de tokens de entrenamiento (es decir, la cantidad de datos de texto con los que se alimenta el modelo) es tan importante como escalar el tamaño del modelo.
8. Dado un presupuesto de cálculo fijo, los investigadores deben asignarlo en proporciones similares para aumentar el tamaño del modelo y el número de tokens de entrenamiento para alcanzar el modelo óptimo de cálculo (medido por la pérdida mínima de entrenamiento). "Por cada duplicación del tamaño del modelo, también debería duplicarse el número de tokens de entrenamiento". Esto implica que un modelo más pequeño puede superar ampliamente a un modelo más grande -pero subóptimo- si se entrena con un número significativamente mayor de tokens.
Chinchilla, a 70B-parameter model 4 times smaller than the previous leader in language AI, Gopher (also built by DeepMind), but trained on 4 times more data.
So, now, the quality of the NLM are measured both in size (number of parameters) as in training tokens. 
Meanwhile LaMDA have 137B but 168B training tokens, Chichilla has 70B and 1.4T training tokens.